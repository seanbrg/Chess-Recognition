{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3: Transfer Learning with ResNet\n",
    "Here we will use ResNet, a pretrained CNN with a unique architecture that allows it to retain some information from the original features after each convolutional layer.\n",
    "\n",
    "We will utilize it for transfer learning just like we saw in the previous notebook, both for CIFAR-10 and our custom dataset, and compare its performance with our own solution."
   ],
   "metadata": {
    "id": "X-BZE3R7EMbk"
   },
   "id": "X-BZE3R7EMbk"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "id": "2fuYsMR_EnlX"
   },
   "id": "2fuYsMR_EnlX"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import cv2\n",
    "import utils\n",
    "from pathlib import Path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "chess_transforms = transforms.Compose([\n",
    "    # Resize to 32x32 to match the model's input\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    # Same normalization stats we used during CIFAR-10 training\n",
    "    transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784))\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784))\n",
    "])\n",
    "\n",
    "# CIFAR-10\n",
    "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "full_test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "val_data = torch.utils.data.Subset(full_test_data, indices=list(range(5000)))\n",
    "test_data = torch.utils.data.Subset(full_test_data, indices=list(range(5000, 10000)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "cifar_trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "cifar_valloader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "cifar_testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ],
   "metadata": {
    "id": "0yO8uNjUE7aw"
   },
   "id": "0yO8uNjUE7aw",
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing ResNet50"
   ],
   "metadata": {
    "id": "yGMPOL1DFr9c"
   },
   "id": "yGMPOL1DFr9c"
  },
  {
   "cell_type": "code",
   "source": [
    "resnet = models.resnet50(weights='DEFAULT')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Freeze the entire body (The \"Warmup\" prep)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the head\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 10) # 10 classes for the CIFAR-10 dataset\n",
    "\n",
    "resnet = resnet.to(device)"
   ],
   "metadata": {
    "id": "URN3ILPjFt35"
   },
   "id": "URN3ILPjFt35",
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Evaluation functions from before"
   ],
   "metadata": {
    "id": "tPUn3BroI2sg"
   },
   "id": "tPUn3BroI2sg"
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, testloader, criterion, device):\n",
    "    model.eval()  # Set to evaluation mode (turns off Dropout/BatchNorm)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # No gradient calculation saved (saves memory)\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    loss = test_loss / len(testloader)\n",
    "    acc = 100. * correct / len(testloader.dataset)\n",
    "    return loss, acc"
   ],
   "metadata": {
    "id": "pnYD2L_sI2OA"
   },
   "id": "pnYD2L_sI2OA",
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_CNN(model, trainloader, valloader, criterion, optimizer, device, epochs=5,\n",
    "               console=False, early_stopping=False, patience=3, scheduler=None):\n",
    "    model.to(device)\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': []\n",
    "    }\n",
    "    best_val_loss = float('inf') # For early stopping\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    no_improvement_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        if scheduler:\n",
    "          scheduler.step()\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        epoch_train_loss = running_loss / len(trainloader)\n",
    "        epoch_train_acc = 100. * correct / total\n",
    "        epoch_val_loss, epoch_val_acc = evaluate(model, valloader, criterion, device)\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "        if console:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "                  f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
    "\n",
    "        if early_stopping:\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                # Save the best model weights\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                no_improvement_counter = 0 # Reset counter\n",
    "            else:\n",
    "                no_improvement_counter += 1\n",
    "                if console: print(f\"  EarlyStopping counter: {no_improvement_counter} out of {patience}\")\n",
    "\n",
    "                if no_improvement_counter >= patience:\n",
    "                    if console: print(\"Early stopping triggered! Restoring best weights...\")\n",
    "                    model.load_state_dict(best_model_wts) # Restore best model\n",
    "                    break\n",
    "\n",
    "    if console: print(\"Finished Training\")\n",
    "    return history"
   ],
   "metadata": {
    "id": "Z1nbTF1WI9L4"
   },
   "id": "Z1nbTF1WI9L4",
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(resnet.fc)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSBBIYlFMMaq",
    "outputId": "da65c8a1-e4d2-4a66-82df-a5278b9f89b0"
   },
   "id": "HSBBIYlFMMaq",
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=2048, out_features=10, bias=True)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before evaluating ResNet on CIFAR-10 we will fine-tune, by freezing it except for its classifier head, and then training it with low learning rate."
   ],
   "metadata": {
    "id": "w1LApMZcIDUD"
   },
   "id": "w1LApMZcIDUD"
  },
  {
   "cell_type": "code",
   "source": [
    "# Unfreeze the last group of residual blocks\n",
    "for param in resnet.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "total_params = sum(p.numel() for p in resnet.parameters())\n",
    "trainable_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()),\n",
    "                             lr=1e-5, weight_decay=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results = train_CNN(resnet, cifar_trainloader, cifar_valloader, criterion, optimizer, device, epochs=5, console=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5d2bGSsIqil",
    "outputId": "d75c804a-083a-450f-9b17-80853bc97632"
   },
   "id": "B5d2bGSsIqil",
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Parameters: 23,528,522\n",
      "Trainable Parameters: 14,985,226\n",
      "Epoch 1/5 | Train Loss: 2.2554 | Train Acc: 16.62% | Val Loss: 2.1561 | Val Acc: 25.12%\n",
      "Epoch 2/5 | Train Loss: 2.0285 | Train Acc: 31.17% | Val Loss: 1.9643 | Val Acc: 37.72%\n",
      "Epoch 3/5 | Train Loss: 1.7968 | Train Acc: 41.05% | Val Loss: 1.8591 | Val Acc: 45.18%\n",
      "Epoch 4/5 | Train Loss: 1.6090 | Train Acc: 47.60% | Val Loss: 1.5403 | Val Acc: 51.50%\n",
      "Epoch 5/5 | Train Loss: 1.4540 | Train Acc: 52.33% | Val Loss: 1.4197 | Val Acc: 54.94%\n",
      "Finished Training\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Unfreeze the whole model\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "total_params = sum(p.numel() for p in resnet.parameters())\n",
    "trainable_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()),\n",
    "                             lr=1e-4, weight_decay=2e-5)\n",
    "cos_scheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results = train_CNN(resnet, cifar_trainloader, cifar_valloader, criterion, optimizer, device, epochs=15,\n",
    "                    console=True, early_stopping=True, patience=5, scheduler=cos_scheduler)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZG1SfzFRNjPG",
    "outputId": "1b77f89b-0177-4b10-a7ad-39a289054a90"
   },
   "id": "ZG1SfzFRNjPG",
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Parameters: 23,528,522\n",
      "Trainable Parameters: 23,528,522\n",
      "Epoch 1/15 | Train Loss: 0.8501 | Train Acc: 71.30% | Val Loss: 0.6076 | Val Acc: 80.48%\n",
      "Epoch 2/15 | Train Loss: 0.4610 | Train Acc: 84.46% | Val Loss: 0.5025 | Val Acc: 83.68%\n",
      "Epoch 3/15 | Train Loss: 0.2927 | Train Acc: 90.10% | Val Loss: 0.6542 | Val Acc: 82.64%\n",
      "  EarlyStopping counter: 1 out of 5\n",
      "Epoch 4/15 | Train Loss: 0.1841 | Train Acc: 93.94% | Val Loss: 0.6927 | Val Acc: 83.88%\n",
      "  EarlyStopping counter: 2 out of 5\n",
      "Epoch 5/15 | Train Loss: 0.1376 | Train Acc: 95.47% | Val Loss: 0.5465 | Val Acc: 84.74%\n",
      "  EarlyStopping counter: 3 out of 5\n",
      "Epoch 6/15 | Train Loss: 0.0872 | Train Acc: 97.21% | Val Loss: 0.6913 | Val Acc: 85.06%\n",
      "  EarlyStopping counter: 4 out of 5\n",
      "Epoch 7/15 | Train Loss: 0.0589 | Train Acc: 98.11% | Val Loss: 0.6153 | Val Acc: 85.94%\n",
      "  EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered! Restoring best weights...\n",
      "Finished Training\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet50 is an extremely powerful model and much larger than the CNN we have built specifically for CIFAR-50. As such, it is easy for it to overfit to the dataset. The simplest way to deal with this problem would be to introduce the same kind of data augmentation we used to train our own Deeper-Wider CNN model."
   ],
   "metadata": {
    "id": "T2I5QVGcQsef"
   },
   "id": "T2I5QVGcQsef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Augmentation"
   ],
   "metadata": {
    "id": "b-4munBqRFLZ"
   },
   "id": "b-4munBqRFLZ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Those are the exact same augmentations we used for our own CNN in notebook 3."
   ],
   "metadata": {
    "id": "paovEXNeRc3H"
   },
   "id": "paovEXNeRc3H"
  },
  {
   "cell_type": "code",
   "source": [
    "transform_augment = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), # Adds a 4px border, then crops a 32x32 square randomly\n",
    "    transforms.RandomHorizontalFlip(), # 50% chance to flip the image horizontally\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784))\n",
    "])\n",
    "\n",
    "train_aug_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_augment)\n",
    "augtrainloader = DataLoader(train_aug_data, batch_size=64, shuffle=True, num_workers=2)"
   ],
   "metadata": {
    "id": "c3skx_vfRELJ"
   },
   "id": "c3skx_vfRELJ",
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model is imported again fresh:"
   ],
   "metadata": {
    "id": "EdUnNUd-SJ8f"
   },
   "id": "EdUnNUd-SJ8f"
  },
  {
   "cell_type": "code",
   "source": [
    "resnet = models.resnet50(weights='DEFAULT')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "resnet = resnet.to(device)"
   ],
   "metadata": {
    "id": "A_s6IW9-R3ST"
   },
   "id": "A_s6IW9-R3ST",
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second Training Attempt"
   ],
   "metadata": {
    "id": "fMuaHuxHSEPD"
   },
   "id": "fMuaHuxHSEPD"
  },
  {
   "cell_type": "code",
   "source": [
    "# Unfreeze the last group of residual blocks\n",
    "for param in resnet.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainable_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()),\n",
    "                             lr=1e-5, weight_decay=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results = train_CNN(resnet, cifar_trainloader, cifar_valloader, criterion, optimizer, device, epochs=5, console=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmSysR4dR994",
    "outputId": "1ac98406-218d-454c-a211-bb88e3882d18"
   },
   "id": "BmSysR4dR994",
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Parameters: 23,528,522\n",
      "Trainable Parameters: 14,985,226\n",
      "Epoch 1/5 | Train Loss: 2.2538 | Train Acc: 16.25% | Val Loss: 2.1623 | Val Acc: 24.38%\n",
      "Epoch 2/5 | Train Loss: 2.0171 | Train Acc: 30.84% | Val Loss: 1.9155 | Val Acc: 38.26%\n",
      "Epoch 3/5 | Train Loss: 1.7897 | Train Acc: 41.29% | Val Loss: 1.7305 | Val Acc: 45.84%\n",
      "Epoch 4/5 | Train Loss: 1.6017 | Train Acc: 47.65% | Val Loss: 1.5198 | Val Acc: 51.72%\n",
      "Epoch 5/5 | Train Loss: 1.4469 | Train Acc: 52.40% | Val Loss: 1.3560 | Val Acc: 54.92%\n",
      "Finished Training\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Unfreeze the whole model\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "total_params = sum(p.numel() for p in resnet.parameters())\n",
    "trainable_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()),\n",
    "                             lr=1e-4, weight_decay=2e-5)\n",
    "cos_scheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results = train_CNN(resnet, cifar_trainloader, cifar_valloader, criterion, optimizer, device, epochs=15,\n",
    "                    console=True, early_stopping=True, patience=5, scheduler=cos_scheduler)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "woi_5iK2SDkH",
    "outputId": "72482ea8-11b8-4d64-9b51-8a5c3f524470"
   },
   "id": "woi_5iK2SDkH",
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Parameters: 23,528,522\n",
      "Trainable Parameters: 23,528,522\n",
      "Epoch 1/15 | Train Loss: 0.8420 | Train Acc: 71.53% | Val Loss: 0.5847 | Val Acc: 81.22%\n",
      "Epoch 2/15 | Train Loss: 0.4477 | Train Acc: 84.92% | Val Loss: 0.6756 | Val Acc: 82.46%\n",
      "  EarlyStopping counter: 1 out of 5\n",
      "Epoch 3/15 | Train Loss: 0.2781 | Train Acc: 90.69% | Val Loss: 0.5649 | Val Acc: 83.86%\n",
      "Epoch 4/15 | Train Loss: 0.1785 | Train Acc: 94.02% | Val Loss: 0.5447 | Val Acc: 84.60%\n",
      "Epoch 5/15 | Train Loss: 0.1163 | Train Acc: 96.17% | Val Loss: 0.5732 | Val Acc: 84.14%\n",
      "  EarlyStopping counter: 1 out of 5\n",
      "Epoch 6/15 | Train Loss: 0.0842 | Train Acc: 97.29% | Val Loss: 0.7484 | Val Acc: 85.44%\n",
      "  EarlyStopping counter: 2 out of 5\n",
      "Epoch 7/15 | Train Loss: 0.0599 | Train Acc: 98.09% | Val Loss: 0.8217 | Val Acc: 84.86%\n",
      "  EarlyStopping counter: 3 out of 5\n",
      "Epoch 8/15 | Train Loss: 0.0441 | Train Acc: 98.59% | Val Loss: 0.7161 | Val Acc: 85.32%\n",
      "  EarlyStopping counter: 4 out of 5\n",
      "Epoch 9/15 | Train Loss: 0.0353 | Train Acc: 98.92% | Val Loss: 1.0119 | Val Acc: 84.12%\n",
      "  EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered! Restoring best weights...\n",
      "Finished Training\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, Data Augmentation provided little benefit. ResNet50 was designed for 224x224 images, and even with the augmentation that we've added it still isn't enough to give the model's huge structure enough to learn to generalize. The core problem is simply not complex enough for ResNet50.\n",
    "\n",
    "We can attempt to solve this by adding a Dropout layer to the head of the model."
   ],
   "metadata": {
    "id": "Q7VhOedOTTAD"
   },
   "id": "Q7VhOedOTTAD"
  },
  {
   "cell_type": "code",
   "source": [
    "resnet = models.resnet50(weights='DEFAULT')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(num_ftrs, 10)\n",
    "    )\n",
    "\n",
    "resnet = resnet.to(device)"
   ],
   "metadata": {
    "id": "Z1nleM9VT1UP"
   },
   "id": "Z1nleM9VT1UP",
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Third Training Attempt"
   ],
   "metadata": {
    "id": "M827nP-EUG02"
   },
   "id": "M827nP-EUG02"
  },
  {
   "cell_type": "code",
   "source": [
    "# Unfreeze the last group of residual blocks\n",
    "for param in resnet.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainable_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()),\n",
    "                             lr=1e-5, weight_decay=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results = train_CNN(resnet, cifar_trainloader, cifar_valloader, criterion, optimizer, device, epochs=5, console=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Uutl_48UAbv",
    "outputId": "b65de59a-0add-46c5-f458-c3ba9e0fa098"
   },
   "id": "-Uutl_48UAbv",
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Parameters: 23,528,522\n",
      "Trainable Parameters: 14,985,226\n",
      "Epoch 1/5 | Train Loss: 2.2831 | Train Acc: 14.15% | Val Loss: 2.2037 | Val Acc: 20.84%\n",
      "Epoch 2/5 | Train Loss: 2.1100 | Train Acc: 24.72% | Val Loss: 2.0222 | Val Acc: 33.22%\n",
      "Epoch 3/5 | Train Loss: 1.9205 | Train Acc: 34.40% | Val Loss: 1.8288 | Val Acc: 42.24%\n",
      "Epoch 4/5 | Train Loss: 1.7424 | Train Acc: 41.49% | Val Loss: 1.6591 | Val Acc: 47.90%\n",
      "Epoch 5/5 | Train Loss: 1.5920 | Train Acc: 46.89% | Val Loss: 1.5680 | Val Acc: 52.52%\n",
      "Finished Training\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Unfreeze the whole model\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "total_params = sum(p.numel() for p in resnet.parameters())\n",
    "trainable_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()),\n",
    "                             lr=1e-4, weight_decay=2e-5)\n",
    "cos_scheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results = train_CNN(resnet, cifar_trainloader, cifar_valloader, criterion, optimizer, device, epochs=15,\n",
    "                    console=True, early_stopping=True, patience=5, scheduler=cos_scheduler)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qkl6OWsKUCZQ",
    "outputId": "ddadebf3-5314-4f31-a75a-12eeef846835"
   },
   "id": "Qkl6OWsKUCZQ",
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Parameters: 23,528,522\n",
      "Trainable Parameters: 23,528,522\n",
      "Epoch 1/15 | Train Loss: 0.9368 | Train Acc: 68.42% | Val Loss: 0.6708 | Val Acc: 79.20%\n",
      "Epoch 2/15 | Train Loss: 0.5136 | Train Acc: 82.74% | Val Loss: 0.5997 | Val Acc: 82.66%\n",
      "Epoch 3/15 | Train Loss: 0.3402 | Train Acc: 88.52% | Val Loss: 0.6370 | Val Acc: 83.66%\n",
      "  EarlyStopping counter: 1 out of 5\n",
      "Epoch 4/15 | Train Loss: 0.2345 | Train Acc: 92.29% | Val Loss: 0.5263 | Val Acc: 85.30%\n",
      "Epoch 5/15 | Train Loss: 0.1623 | Train Acc: 94.63% | Val Loss: 0.5874 | Val Acc: 85.52%\n",
      "  EarlyStopping counter: 1 out of 5\n",
      "Epoch 6/15 | Train Loss: 0.1136 | Train Acc: 96.26% | Val Loss: 0.6982 | Val Acc: 85.16%\n",
      "  EarlyStopping counter: 2 out of 5\n",
      "Epoch 7/15 | Train Loss: 0.0813 | Train Acc: 97.35% | Val Loss: 0.7325 | Val Acc: 85.12%\n",
      "  EarlyStopping counter: 3 out of 5\n",
      "Epoch 8/15 | Train Loss: 0.0566 | Train Acc: 98.17% | Val Loss: 0.5916 | Val Acc: 86.32%\n",
      "  EarlyStopping counter: 4 out of 5\n",
      "Epoch 9/15 | Train Loss: 0.0449 | Train Acc: 98.57% | Val Loss: 0.6642 | Val Acc: 86.60%\n",
      "  EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered! Restoring best weights...\n",
      "Finished Training\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusions\n",
    "The model reached 85% accuracy on the validation set regardless of what we did with the data.\n",
    "\n",
    "ResNet50 is overfitting on CIFAR-10, which is hard to solve without more aggressive regularization and more tampering of its architecture. For this reason we are certain that it would overfit much harder on the chess piece dataset, which is smaller and simpler, even for our simple CNN.\n",
    "\n",
    " As we've seen, ResNet50 has worse performance on smaller image datasets than other models that have been selectively designed for such datasets (like our DW-CNN which reached 90% accuracy on CIFAR-10 under the same conditions). This seeming paradox is because ResNet50's 'receptive field' is larger than the image itself and it is capable of memorizing entire images as 'high level features', something that a smaller model like the CNN we have constructed is unable to do and is thus forced to look for more generalizing patterns."
   ],
   "metadata": {
    "id": "tBdfP16eVXew"
   },
   "id": "tBdfP16eVXew"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "H100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
